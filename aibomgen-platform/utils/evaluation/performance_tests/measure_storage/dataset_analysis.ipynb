{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "362179cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted dataset size 100: 200 {\"job_id\":\"f3f2c8ff-88c6-4fca-b611-4259d5eced07\",\"status\":\"Training started\",\"unique_dir\":\"d1a63588-a0a3-4e7e-8b45-7350325dff60\"}\n",
      "Submitted dataset size 500: 200 {\"job_id\":\"78bf9a35-1db4-46e5-900d-01c4d7141267\",\"status\":\"Training started\",\"unique_dir\":\"a8c208b1-d5d6-45b3-8004-570ae86e745d\"}\n",
      "Submitted dataset size 1000: 200 {\"job_id\":\"3101e6a5-dbb2-4550-89f2-5dcc063ec18f\",\"status\":\"Training started\",\"unique_dir\":\"0f1b81c2-dffe-4db1-b2f8-dfc42b6f881a\"}\n"
     ]
    }
   ],
   "source": [
    "# --- Request jobs for increasing dataset sizes ---\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "API_URL = \"http://localhost:8000/developer/submit_job_by_model_and_data\"\n",
    "DOWNLOADS = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "\n",
    "def create_model():\n",
    "    # Use a fixed model for all dataset sizes\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(32, 32, 3)),\n",
    "        tf.keras.layers.Conv2D(8, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def save_images_per_class(x, y, base_folder):\n",
    "    os.makedirs(base_folder, exist_ok=True)\n",
    "    class_counts = {i: 0 for i in range(10)}\n",
    "    for class_idx in range(10):\n",
    "        os.makedirs(os.path.join(base_folder, f\"class_{class_idx}\"), exist_ok=True)\n",
    "    for i in range(len(x)):\n",
    "        label = int(y[i])\n",
    "        img_uint8 = (x[i] * 255).astype(np.uint8)\n",
    "        img_pil = Image.fromarray(img_uint8)\n",
    "        img_path = os.path.join(base_folder, f\"class_{label}\", f\"image_{class_counts[label]}.png\")\n",
    "        img_pil.save(img_path)\n",
    "        class_counts[label] += 1\n",
    "\n",
    "# Choose your dataset sizes\n",
    "dataset_sizes = [100, 500, 1000]\n",
    "job_ids = []\n",
    "\n",
    "for size in dataset_sizes:\n",
    "    # Generate random dataset\n",
    "    x = np.random.rand(size, 32, 32, 3).astype(np.float32)\n",
    "    y = np.random.randint(0, 10, size).astype(np.int64)\n",
    "    dataset_folder = os.path.join(DOWNLOADS, f\"dataset_analysis_{size}\")\n",
    "    save_images_per_class(x, y, dataset_folder)\n",
    "\n",
    "    # YAML definition\n",
    "    yaml_path = os.path.join(DOWNLOADS, f\"dataset_analysis_{size}_definition.yaml\")\n",
    "    dataset_definition = {\n",
    "        \"type\": \"image\",\n",
    "        \"image_size\": [32, 32],\n",
    "        \"input_shape\": [32, 32, 3],\n",
    "        \"output_shape\": [10],\n",
    "        \"preprocessing\": {\"normalize\": True}\n",
    "    }\n",
    "    with open(yaml_path, \"w\") as f:\n",
    "        yaml.dump(dataset_definition, f)\n",
    "\n",
    "    # Save fixed model (only once)\n",
    "    model_path = os.path.join(DOWNLOADS, \"dataset_analysis_model.keras\")\n",
    "    if not os.path.exists(model_path):\n",
    "        model = create_model()\n",
    "        model.save(model_path)\n",
    "\n",
    "    # Zip dataset\n",
    "    shutil.make_archive(dataset_folder, 'zip', dataset_folder)\n",
    "    dataset_zip = dataset_folder + \".zip\"\n",
    "\n",
    "    files = {\n",
    "        \"model\": open(model_path, \"rb\"),\n",
    "        \"dataset\": open(dataset_zip, \"rb\"),\n",
    "        \"dataset_definition\": open(yaml_path, \"rb\"),\n",
    "    }\n",
    "    data = {\n",
    "        \"framework\": \"TensorFlow 2.16.1\",\n",
    "        \"model_name\": f\"dataset_analysis_model\",\n",
    "        \"epochs\": 1,\n",
    "    }\n",
    "    response = requests.post(API_URL, files=files, data=data)\n",
    "    print(f\"Submitted dataset size {size}: {response.status_code} {response.text}\")\n",
    "    job_id = response.json().get(\"job_id\")\n",
    "    job_ids.append(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c2e48",
   "metadata": {},
   "source": [
    "Download the job folders from MinIO for analysis !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37726d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# List your dataset sizes and corresponding job folders here\n",
    "dataset_sizes = [100, 500, 1000]  # Example: number of images in each dataset\n",
    "job_folders = [\n",
    "    \"d1a63588-a0a3-4e7e-8b45-7350325dff60\",   # replace with actual folder name for 100 images\n",
    "    \"a8c208b1-d5d6-45b3-8004-570ae86e745d\",   # replace with actual folder name for 500 images\n",
    "    \"0f1b81c2-dffe-4db1-b2f8-dfc42b6f881a\",  # replace with actual folder name for 1000 images\n",
    "]\n",
    "\n",
    "DOWNLOADS = Path.home() / \"Downloads\"\n",
    "\n",
    "input_model_sizes = []\n",
    "trained_model_sizes = []\n",
    "aibom_sizes = []\n",
    "dataset_zip_sizes = []\n",
    "definition_sizes = []\n",
    "logs_sizes = []\n",
    "metrics_sizes = []\n",
    "runlink_sizes = []\n",
    "total_folder_sizes = []\n",
    "\n",
    "for size, folder in zip(dataset_sizes, job_folders):\n",
    "    folder_path = DOWNLOADS / folder\n",
    "    dataset_file = folder_path / \"dataset\" / f\"dataset_analysis_{size}.zip\"\n",
    "    definition_file = folder_path / \"definition\" / f\"dataset_analysis_{size}_definition.yaml\"\n",
    "    input_model_file = folder_path / \"model\" / \"dataset_analysis_model.zip\"  # or whatever fixed model you used\n",
    "    output_path = folder_path / \"output\"\n",
    "    trained_model_file = output_path / \"trained_model.keras\"\n",
    "    aibom_file = output_path / \"cyclonedx_bom.json\"\n",
    "    logs_file = output_path / \"logs.log\"\n",
    "    metrics_file = output_path / \"metrics.json\"\n",
    "    runlink_file = next(output_path.glob(\"run_training.*.link\"), None)\n",
    "\n",
    "    dataset_zip_sizes.append(dataset_file.stat().st_size / 1024 if dataset_file.exists() else None)\n",
    "    definition_sizes.append(definition_file.stat().st_size / 1024 if definition_file.exists() else None)\n",
    "    input_model_sizes.append(input_model_file.stat().st_size / 1024 if input_model_file.exists() else None)\n",
    "    trained_model_sizes.append(trained_model_file.stat().st_size / 1024 if trained_model_file.exists() else None)\n",
    "    aibom_sizes.append(aibom_file.stat().st_size / 1024 if aibom_file.exists() else None)\n",
    "    logs_sizes.append(logs_file.stat().st_size / 1024 if logs_file.exists() else None)\n",
    "    metrics_sizes.append(metrics_file.stat().st_size / 1024 if metrics_file.exists() else None)\n",
    "    runlink_sizes.append(runlink_file.stat().st_size / 1024 if runlink_file and runlink_file.exists() else None)\n",
    "\n",
    "    # Calculate total folder size in KB\n",
    "    total_size = sum(f.stat().st_size for f in folder_path.rglob('*') if f.is_file()) / 1024\n",
    "    total_folder_sizes.append(total_size)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Dataset Size\": dataset_sizes,\n",
    "    \"Dataset Zip Size (KB)\": dataset_zip_sizes,\n",
    "    \"Definition Size (KB)\": definition_sizes,\n",
    "    \"Input Model Size (KB)\": input_model_sizes,\n",
    "    \"Trained Model Size (KB)\": trained_model_sizes,\n",
    "    \"AIBOM Size (KB)\": aibom_sizes,\n",
    "    \"Logs Size (KB)\": logs_sizes,\n",
    "    \"Metrics Size (KB)\": metrics_sizes,\n",
    "    \"RunLink Size (KB)\": runlink_sizes,\n",
    "    \"Total Job Folder Size (KB)\": total_folder_sizes,\n",
    "    \"AIBOM % of Trained Model\": [100 * a / m if m else None for a, m in zip(aibom_sizes, trained_model_sizes)]\n",
    "})\n",
    "print(df)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(df[\"Dataset Size\"], df[\"Input Model Size (KB)\"], marker='o', label=\"Input Model\")\n",
    "plt.plot(df[\"Dataset Size\"], df[\"Trained Model Size (KB)\"], marker='o', label=\"Trained Model\")\n",
    "plt.plot(df[\"Dataset Size\"], df[\"AIBOM Size (KB)\"], marker='o', label=\"AIBOM\")\n",
    "plt.plot(df[\"Dataset Size\"], df[\"Metrics Size (KB)\"], marker='o', label=\"Metrics\")\n",
    "plt.plot(df[\"Dataset Size\"], df[\"Dataset Zip Size (KB)\"], marker='o', label=\"Dataset Zip\")\n",
    "plt.plot(df[\"Dataset Size\"], df[\"Definition Size (KB)\"], marker='o', label=\"Definition\")\n",
    "plt.plot(df[\"Dataset Size\"], df[\"Logs Size (KB)\"], marker='o', label=\"Logs\")\n",
    "plt.plot(df[\"Dataset Size\"], df[\"RunLink Size (KB)\"], marker='o', label=\"RunLink\")\n",
    "plt.plot(df[\"Dataset Size\"], df[\"Total Job Folder Size (KB)\"], marker='o', label=\"Total Job Folder\")\n",
    "\n",
    "plt.xlabel(\"Dataset Size (number of images)\")\n",
    "plt.ylabel(\"File Size (KB)\")\n",
    "plt.title(\"File Sizes vs Dataset Size for All Artifacts (Log Scale)\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
